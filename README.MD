# N-gram Language Model

This repository contains a simple yet efficient implementation of an N-gram language model using Python. The model is built using the text of "Frankenstein" by Mary Shelley, obtained from Project Gutenberg. The primary goal is to predict the next word in a sequence based on the previous N-1 words, employing a custom tokenizer and efficient code to minimize computation time and enhance inference performance.

# Features

- Tokenization: Custom tokenizer to split sentences into words.
- N-gram Generation: Function to create N-grams from tokenized text.
- Handling Start Tags: Use of <START> tags to manage the beginning of sentences.
- Efficient Inference: Optimized code to reduce computation time and enhance inference efficiency.

# using 

# Contributing
Contributions are welcome! If you have any suggestions or improvements, please submit a pull request or open an issue.

# License
This project is licensed under the MIT License.